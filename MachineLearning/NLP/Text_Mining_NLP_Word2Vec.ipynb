{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec and FastText Word Embedding with Gensim\n",
    "\n",
    "import gzip\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=\"reviews_data.gz\"\n",
    "\n",
    "with gzip.open ('reviews_data.gz', 'r') as f:\n",
    "    review_data=[ line for i,line in enumerate (f)]\n",
    "    \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample dataset\n",
    "review_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    \n",
    "# Clean and preprocess the dataset\n",
    "\n",
    "def preprocess(dataset):\n",
    "    #lowercase and tokenize\n",
    "    review_data_tokenize= [word_tokenize(str(review).lower()) for review in review_data]\n",
    "    \n",
    "    #remove stopwords\n",
    "    sr = stopwords.words('english')\n",
    "    review_data_cleaned= [token for token in review_data_tokenize if not token in sr]\n",
    "    \n",
    "    \n",
    "    # Stemming and Lemmatization\n",
    "    stemmer=PorterStemmer()\n",
    "    review_data_stems=[stemmer.stem(token) for token in review_data_cleaned ]\n",
    "    \n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    review_data_processed=[lemmatizer.lematize(token) for token in review_data_cleaned ]\n",
    "    \n",
    "    \n",
    "    return review_data_processed\n",
    "\n",
    "\n",
    "# Clean and preprocess the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tall', 'tall', 'grandiloquent', 'magniloquent', 'tall', 'tall', 'improbable', 'marvelous', 'marvellous', 'tall']\n",
      "['short']\n"
     ]
    }
   ],
   "source": [
    "# Synonyms, Antonyms\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets('tall'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "        \n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print(synonyms)\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  and/CC\n",
      "  (PERSON John/NNP)\n",
      "  are/VBP\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# POS and NER\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "sentence = \"Mark and John are working at Google.\"\n",
    " \n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f):\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "            #yield preprocess(line)\n",
    "\n",
    "\n",
    "documents = list (read_input (data_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303494708, 415193550)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Word2Vec\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('filthy', 0.8731397390365601),\n",
       " ('unclean', 0.7831881642341614),\n",
       " ('stained', 0.7769122123718262),\n",
       " ('dusty', 0.7751280665397644),\n",
       " ('grubby', 0.760492742061615),\n",
       " ('smelly', 0.758076548576355),\n",
       " ('dingy', 0.7364238500595093),\n",
       " ('mouldy', 0.7207521200180054),\n",
       " ('gross', 0.7180871367454529),\n",
       " ('soiled', 0.7178699970245361)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Similar words\n",
    "w1 = \"dirty\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7580765030398822"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Words similarity\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# odd one out \n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec Mathematics\n",
    "\n",
    "def A_is_to_B_as_C_is_to(a, b, c, topn=1):\n",
    "    a, b, c = map(lambda x:x if type(x) == list else [x], (a, b, c))\n",
    "    res = model.most_similar(positive=b + c, negative=a, topn=topn)\n",
    "    if len(res):\n",
    "        if topn == 1:\n",
    "            return res[0][0]\n",
    "        return [x[0] for x in res]\n",
    "    return None\n",
    "\n",
    "A_is_to_B_as_C_is_to('man', 'woman', 'king')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FastText\n",
    "\n",
    "from gensim.models import FastText\n",
    "model_ft = FastText(documents, size=100, window=5, min_count=5, workers=4,sg=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('catastrophy', 0.7369320392608643),\n",
       " ('desastroso', 0.7364808320999146),\n",
       " ('granduer', 0.7288857698440552),\n",
       " ('struttura', 0.7264410257339478),\n",
       " ('katastrophe', 0.7233595848083496),\n",
       " ('catastrophe', 0.7171571254730225),\n",
       " ('xixeme', 0.7164624333381653),\n",
       " ('schizophrenic', 0.7157965898513794),\n",
       " ('bohme', 0.7148301601409912),\n",
       " ('beind', 0.7146252393722534)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fast text model can give result for non eistence word in the vocabulary as well\n",
    "model_ft.wv.most_similar(\"Gastroenteritis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hotell', 0.9006769061088562),\n",
       " ('hotelhas', 0.8679174184799194),\n",
       " ('hotelthis', 0.8461465239524841),\n",
       " ('hotelbut', 0.8390347957611084),\n",
       " ('hotelwas', 0.8342684507369995),\n",
       " ('hotelwith', 0.8297032117843628),\n",
       " ('hotelat', 0.8279556035995483),\n",
       " ('shotel', 0.8165179491043091),\n",
       " ('hotelgood', 0.8164716958999634),\n",
       " ('hotelwould', 0.8118799924850464)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
