{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation,TruncatedSVD\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank you Louisville, Kentucky- on my way! #MAGAðŸ‡ºðŸ‡¸ -']\n",
      "['Thank you Louisville, Kentucky- on my way! #MAGAðŸ‡ºðŸ‡¸ -']\n"
     ]
    }
   ],
   "source": [
    "#Data import and preprocessing\n",
    "df=pd.read_csv('twitter_data.txt', sep = \"\\n\", header=None, names=[\"content\"],error_bad_lines=False)\n",
    "\n",
    "\n",
    "data=df.content.values.tolist()\n",
    "\n",
    "#Basic cleaning\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data_words = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "print(data_words[:1])\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['que','los','don','htt','https','por','la','al','se','le','el','30','10','del','amp'])\n",
    "\n",
    "cleaned_data=[data for data in data_words if not data in stop_words]\n",
    "print(cleaned_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33107, 8)\n",
      "(33107, 8)\n",
      "(33107, 8)\n"
     ]
    }
   ],
   "source": [
    "#Modelling\n",
    "no_features=100\n",
    "NUM_TOPICS=8\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "count_vectorized = count_vectorizer.fit_transform(cleaned_data)\n",
    "count_feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "# NMF can use both countvector or  tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf_vectorized = tfidf_vectorizer.fit_transform(cleaned_data)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    " \n",
    "# Build a Latent Dirichlet Allocation Model\n",
    "lda_model = LatentDirichletAllocation(n_topics=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "lda_Z = lda_model.fit_transform(count_vectorized)\n",
    "print(lda_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n",
    "# Build a Non-Negative Matrix Factorization Model\n",
    "nmf_model = NMF(n_components=NUM_TOPICS)\n",
    "nmf_Z = nmf_model.fit_transform(tfidf_vectorized)\n",
    "print(nmf_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n",
    "# Build a Latent Semantic Indexing Model\n",
    "lsi_model = TruncatedSVD(n_components=NUM_TOPICS)\n",
    "lsi_Z = lsi_model.fit_transform(count_vectorized)\n",
    "print(lsi_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0625 0.0625 0.0625 0.0625 0.5625 0.0625 0.0625 0.0625]\n",
      "[0.00012822 0.00019631 0.00023452 0.00092445 0.00170599 0.00022699\n",
      " 0.00077469 0.00319967]\n",
      "[ 0.00716199 -0.00019003 -0.00231519  0.01191622  0.0128451  -0.00113862\n",
      " -0.00405776 -0.01139345]\n"
     ]
    }
   ],
   "source": [
    "#first document in the corpus looks like in different topic spaces\n",
    "print(lda_Z[0])\n",
    "print(nmf_Z[0])\n",
    "print(lsi_Z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display topic word distribution\n",
    "def display_topics(model, vectorizer, top_n=5):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n",
      "Topic 0:\n",
      "[('amp', 1773.6932405424664), ('rt', 1515.6760970453167), ('new', 1194.3209124155285), ('york', 289.6567359642981), ('music', 269.4999420674596)]\n",
      "Topic 1:\n",
      "[('https', 15951.753959375315), ('time', 420.9189787443214), ('day', 397.4318379919141), ('news', 376.3236005449662), ('american', 330.16749123842317)]\n",
      "Topic 2:\n",
      "[('https', 1031.283954465345), ('rt', 456.13435829626764), ('great', 352.6649312973796), ('trump', 338.04208682701324), ('love', 337.7197282626997)]\n",
      "Topic 3:\n",
      "[('https', 1635.634019949097), ('la', 975.3336418581357), ('rt', 916.6038888066778), ('en', 724.3644596128928), ('el', 465.3504368534968)]\n",
      "Topic 4:\n",
      "[('https', 1230.1250002952183), ('just', 531.1078133861898), ('today', 408.53077748280657), ('john', 343.2986805685746), ('good', 293.4135604796725)]\n",
      "Topic 5:\n",
      "[('rt', 8232.056848697353), ('big', 217.15915451860909), ('going', 184.75444047397775), ('party', 183.65864880712726), ('11', 168.80534627135776)]\n",
      "Topic 6:\n",
      "[('like', 557.3336371004631), ('gt', 392.39715232837636), ('people', 352.57510229882564), ('national', 303.3161747921523), ('dont', 280.5397916623639)]\n",
      "Topic 7:\n",
      "[('https', 1249.2707583957435), ('2017', 656.9530195231429), ('im', 417.6862124535137), ('university', 349.7940417308313), ('city', 348.98406905926805)]\n",
      "===============\n",
      "NMF Model:\n",
      "Topic 0:\n",
      "[('https', 9.77986543272184), ('news', 0.06781709370934791), ('video', 0.05400738248172405), ('university', 0.04110547526214557), ('john', 0.03989485844626154)]\n",
      "Topic 1:\n",
      "[('rt', 7.973073881543364), ('today', 0.062392938838496184), ('university', 0.06225124867250975), ('years', 0.05355226647053155), ('people', 0.053151793744727525)]\n",
      "Topic 2:\n",
      "[('amp', 5.294577932691863), ('food', 0.104838773211505), ('music', 0.10155549385898134), ('work', 0.09086703132460172), ('today', 0.09002487396371692)]\n",
      "Topic 3:\n",
      "[('new', 4.655180794943341), ('york', 1.0015739421647842), ('city', 0.24933198866688078), ('today', 0.18746617161538628), ('year', 0.17663601888646754)]\n",
      "Topic 4:\n",
      "[('la', 3.945227587630238), ('en', 2.013847882062126), ('el', 1.0069090897896196), ('que', 0.5203220161332044), ('le', 0.4573132361231469)]\n",
      "Topic 5:\n",
      "[('2017', 4.424833789185405), ('world', 0.13650570741369922), ('10', 0.12756213901298485), ('live', 0.1219102136000178), ('read', 0.09445028141408396)]\n",
      "Topic 6:\n",
      "[('like', 4.524186979326842), ('people', 0.28472651977516733), ('know', 0.2509361000443394), ('dont', 0.24361392267153895), ('love', 0.2323260593590425)]\n",
      "Topic 7:\n",
      "[('just', 3.671543447607798), ('im', 0.5978703772985294), ('time', 0.44158015080453283), ('good', 0.24090587058504703), ('john', 0.1959464276232053)]\n",
      "===============\n",
      "LSI Model:\n",
      "Topic 0:\n",
      "[('https', 0.9254096590769043), ('rt', 0.3593408841788413), ('new', 0.047625668511677335), ('amp', 0.04590400235461188), ('public', 0.0305011705315039)]\n",
      "Topic 1:\n",
      "[('https', 0.3664989668142571), ('public', 0.02189852471406446), ('art', 0.01973812517937443), ('news', 0.014928745413874412), ('michael', 0.008926862103927914)]\n",
      "Topic 2:\n",
      "[('amp', 0.9668043037895138), ('new', 0.0695733832935092), ('says', 0.054953209775126695), ('art', 0.02685973859782592), ('university', 0.023249061156168097)]\n",
      "Topic 3:\n",
      "[('new', 0.3483024947387544), ('art', 0.10423357791160022), ('public', 0.10242331681503926), ('york', 0.08275392167541629), ('rt', 0.059280539541707915)]\n",
      "Topic 4:\n",
      "[('new', 0.8291791682091618), ('la', 0.3045218479589884), ('art', 0.20231283880291392), ('en', 0.19914859515520794), ('york', 0.19866844144452162)]\n",
      "Topic 5:\n",
      "[('gt', 0.8869689078130786), ('university', 0.2858175315343938), ('state', 0.24799608686671132), ('public', 0.12123297390485585), ('2017', 0.09093799415601941)]\n",
      "Topic 6:\n",
      "[('university', 0.6620839552574859), ('state', 0.6116996508198125), ('just', 0.11114250752699467), ('2017', 0.08904634593577435), ('im', 0.06637562468560826)]\n",
      "Topic 7:\n",
      "[('great', 0.5669922223581664), ('american', 0.5033834987741579), ('im', 0.25297246560300307), ('gt', 0.20798989709294008), ('new', 0.1509045245763799)]\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA Model:\")\n",
    "display_topics(lda_model, count_vectorizer)\n",
    "print(\"=\" * 15)\n",
    "\n",
    "print(\"NMF Model:\")\n",
    "display_topics(nmf_model, tfidf_vectorizer)\n",
    "print(\"=\" * 15)\n",
    " \n",
    "print(\"LSI Model:\")\n",
    "display_topics(lsi_model, count_vectorizer)\n",
    "print(\"=\" * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Transforming an unseen document\n",
    "text = \"The economy is working better than ever\"\n",
    "x = nmf_model.transform(tfidf_vectorizer.transform([text]))[0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida 16-8\n"
     ]
    }
   ],
   "source": [
    "#similarity functionality like gensim\n",
    " \n",
    "def most_similar(x, Z, top_n=5):\n",
    "    dists = euclidean_distances(x.reshape(1, -1), Z)\n",
    "    pairs = enumerate(dists[0])\n",
    "    most_similar = sorted(pairs, key=lambda item: item[1])[:top_n]\n",
    "    return most_similar\n",
    " \n",
    "similarities = most_similar(x, nmf_Z)\n",
    "document_id, similarity = similarities[0]\n",
    "print(data[document_id][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "# visualization\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, count_vectorized, count_vectorizer, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -10.435327106471307\n",
      "\n",
      "Coherence Score:  0.4757935386133725\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics = 2  has Coherence Value of 0.4938\n",
      "Num Topics = 4  has Coherence Value of 0.4483\n",
      "Num Topics = 6  has Coherence Value of 0.4489\n",
      "Num Topics = 8  has Coherence Value of 0.4244\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you Louisville, Kentucky- on my way! #MAGAðŸ‡ºðŸ‡¸ -'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1b048a808851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         print (\" \".join([feature_names[i]\n\u001b[1;32m      8\u001b[0m                         for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mno_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c9fd8890da71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpanel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tsne'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpanel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f6ccb72523b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmf_Z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdocument_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocument_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nmf_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a3c16500519f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The economy is working better than ever\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nmf_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
